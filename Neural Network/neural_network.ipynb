{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Implementation of Neural Network** </center>\n",
    "#### <center> <span style=\"color:gray\"> ***With Stocastic, Mini-Batch and Batch Gradient Descent*** </span> </center>\n",
    "\n",
    "> Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def activate(self, input):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivationFunction(ActivationFunction):\n",
    "    def activate(self, input):\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "    \n",
    "    def gradient(self, input): # batc_size * number_of_inputs * number_of_inputs\n",
    "        activated = self.activate(input)\n",
    "        res =  (1 - activated) * activated\n",
    "        \n",
    "        grad = []\n",
    "        for idx in range(input.shape[1]):\n",
    "            grad.append(np.diag(res[:,idx]))\n",
    "\n",
    "        return np.stack(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def loss(self, target, result):\n",
    "        pass\n",
    "    \n",
    "    def gradient(self, target, result):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquareLoss(LossFunction):\n",
    "    def loss(self, target, result):\n",
    "        l = target - result\n",
    "        l = l * l\n",
    "        res = 0.5 * np.sum(l) / np.size(target)\n",
    "        return res\n",
    "    \n",
    "    def gradient(self, target, result):\n",
    "        return result - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "        \n",
    "    @property\n",
    "    def output(self):\n",
    "        if(self._output is None):\n",
    "            print(\"Output is not calculated yet!\")\n",
    "        return self._output\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input): # input size = (number_of_features X batch_size)\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self, output_gradient, learning_rate : float):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    def __init__(self, number_of_inputs : int, number_of_outputs : int) -> None:\n",
    "        self._number_of_inputs = number_of_inputs\n",
    "        self._number_of_outputs = number_of_outputs\n",
    "        self._weight = np.random.randn(number_of_inputs, number_of_outputs)\n",
    "        self._bias = np.random.randn(number_of_outputs, 1)\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self._input = input\n",
    "        self._output = np.matmul(self._weight.T, input) + self._bias\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate: float):\n",
    "        res =  np.matmul(self._weight, output_gradient)\n",
    "        \n",
    "        gradient_weight = np.matmul(self._input, output_gradient.T)\n",
    "        gradient_bias = output_gradient\n",
    "        self._weight -= learning_rate * gradient_weight\n",
    "        self._bias -= learning_rate * gradient_bias\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation_function : ActivationFunction) -> None:\n",
    "        super().__init__()\n",
    "        self._activation_function = activation_function\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self._input = input\n",
    "        self._output = self._activation_function.activate(self._input)\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate: float):\n",
    "        output_gradient = np.transpose(output_gradient)\n",
    "        output_gradient = np.expand_dims(output_gradient, axis=2)\n",
    "        res = np.matmul(self._activation_function.gradient(self._input), output_gradient)\n",
    "        res = np.squeeze(res)\n",
    "        res = np.transpose(res)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, ) -> None:\n",
    "        self.layers : list[Layer] = []\n",
    "        self.loss_function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
