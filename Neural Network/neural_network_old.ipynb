{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **Implementation of Neural Network** </center>\n",
    "#### <center> ***With Stochastic Gradient Descent*** </center>\n",
    ">- ### Activation Functions\n",
    ">> 1. Sigmoid Activation\n",
    ">> 1. ReLU Activation\n",
    ">> 1. Softmax Activation\n",
    ">- ### Loss Functions\n",
    ">> 1. Mean Square Loss\n",
    ">> 1. Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def activate(self, input):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidActivation(ActivationFunction):\n",
    "    def activate(self, input):\n",
    "        res = 1.0 / (1.0 + np.exp(-input))\n",
    "        return res\n",
    "\n",
    "    def gradient(self, input):\n",
    "        sig = self.activate(input)\n",
    "        sig = (1.0 - sig) * sig\n",
    "        sig = np.squeeze(sig)\n",
    "        return np.diag(sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLUActivation(ActivationFunction):\n",
    "    def activate(self, input):\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def gradient(self, input):\n",
    "        res = self.activate(input)\n",
    "        res = np.squeeze(res)\n",
    "        for i in range(len(res)):\n",
    "            res[i] = 1.0 if res[i] > 0 else 0.0\n",
    "        return np.diag(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxActivation(ActivationFunction):\n",
    "    def activate(self, input):\n",
    "        s = np.sum(np.exp(input))\n",
    "        return np.exp(input) / s\n",
    "    \n",
    "    def gradient(self, input):\n",
    "        try:\n",
    "            input = np.squeeze(input)\n",
    "            s = input.shape\n",
    "            grad = np.zeros((s[0], s[0]), dtype=np.float64)\n",
    "            a = self.activate(input)\n",
    "            for i in range(s[0]):\n",
    "                for j in range(i, s[0]):\n",
    "                    if i == j:\n",
    "                        grad[i][j] = a[i] * (1 - a[i])\n",
    "                    else:\n",
    "                        grad[i][j] = -1 * a[i] * a[j]\n",
    "                        grad[j][i] = grad[i][j]\n",
    "            return grad\n",
    "        except:\n",
    "            return self.activate(input) * (1 - self.activate(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(ABC):\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def loss(self, target, output):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient(self, target, output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSqureLoss(LossFunction):\n",
    "    def loss(self, target, output):\n",
    "        loss = target - output\n",
    "        loss = loss * loss\n",
    "        return 0.5 * np.sum(loss)\n",
    "    \n",
    "    def gradient(self, target, output):\n",
    "        return output - target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(LossFunction):\n",
    "    def loss(self, target, output):\n",
    "        loss = -target * np.log10(output)\n",
    "        return np.sum(loss)\n",
    "    \n",
    "    def gradient(self, target, output):\n",
    "        return - target / output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    def __init__(self, number_of_nodes : int, activation_function : ActivationFunction = None):\n",
    "        self.__number_of_nodes = number_of_nodes\n",
    "        self.__activation_function : ActivationFunction = activation_function\n",
    "    \n",
    "    @property\n",
    "    def number_of_nodes(self):\n",
    "        return self.__number_of_nodes\n",
    "    \n",
    "    \n",
    "    def activate(self, input):\n",
    "        if self.__activation_function:\n",
    "            return self.__activation_function.activate(input)\n",
    "        \n",
    "        return input\n",
    "    \n",
    "    def gradient(self, input):\n",
    "        if self.__activation_function:\n",
    "            return self.__activation_function.gradient(input)\n",
    "        \n",
    "        print(\"No Activation Function is added...\")\n",
    "        return np.ones(input.shape, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, number_of_inputs : int, number_of_outputs : int, loss_function : LossFunction, output_activation_function : ActivationFunction = None) -> None:\n",
    "        self.__number_of_inputs = number_of_inputs\n",
    "        self.__number_of_outputs = number_of_outputs\n",
    "        self.__hidden_layers : list[Layer] = []\n",
    "        self.__loss_function : LossFunction = loss_function\n",
    "        self.__output_layer : Layer = None\n",
    "        if output_activation_function:\n",
    "            self.__output_layer = Layer(number_of_outputs, output_activation_function)\n",
    "        self.__is_initialized = False\n",
    "    \n",
    "    def addHiddenLayer(self, layer : Layer):\n",
    "        self.__is_initialized = False\n",
    "        self.__hidden_layers.append(layer)\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.__is_initialized = True\n",
    "        self.__weights = []\n",
    "        self.__biases = []\n",
    "        self.__weighted_sum = []\n",
    "        self.__activated_output = [] # output after applying activation function in each layer\n",
    "        self.__delta_error = [] # error in each layer\n",
    "        \n",
    "        prev_layer_size = self.__number_of_inputs\n",
    "        for i in range(len(self.__hidden_layers)):\n",
    "            self.__weights.append(np.random.rand(prev_layer_size, self.__hidden_layers[i].number_of_nodes) - 0.5)\n",
    "            self.__biases.append(np.random.rand(self.__hidden_layers[i].number_of_nodes, 1) - 0.5)\n",
    "            \n",
    "            self.__weighted_sum.append(np.zeros((self.__hidden_layers[i].number_of_nodes, 1), dtype=np.float64))\n",
    "            self.__activated_output.append(np.zeros((self.__hidden_layers[i].number_of_nodes, 1), dtype=np.float64))\n",
    "            self.__delta_error.append(np.zeros((self.__hidden_layers[i].number_of_nodes, 1), dtype=np.float64))\n",
    "            prev_layer_size = self.__hidden_layers[i].number_of_nodes\n",
    "            \n",
    "        self.__weights.append(np.random.rand(prev_layer_size, self.__number_of_outputs) - 0.5)\n",
    "        self.__biases.append(np.random.rand(self.__number_of_outputs, 1) - 0.5)\n",
    "\n",
    "        self.__weighted_sum.append(np.zeros((self.__number_of_outputs, 1), dtype=np.float64))\n",
    "        self.__activated_output.append(np.zeros((self.__number_of_outputs, 1), dtype=np.float64))\n",
    "        self.__delta_error.append(np.zeros((self.__number_of_outputs, 1), dtype=np.float64))\n",
    "    \n",
    "    @property\n",
    "    def weights(self):\n",
    "        return self.__weights\n",
    "    \n",
    "    @property\n",
    "    def biases(self):\n",
    "        return self.__biases\n",
    "    \n",
    "    @property\n",
    "    def activated_output(self):\n",
    "        return self.__activated_output\n",
    "    \n",
    "    def initializeWeights(self, weights):\n",
    "        self.__weights = weights\n",
    "        \n",
    "    def initializeBiases(self, biases):\n",
    "        self.__biases = biases\n",
    "    \n",
    "    def __forwardPass(self, input):\n",
    "        if not self.__is_initialized:\n",
    "            self.initialize()\n",
    "        \n",
    "        for i in range(len(self.__hidden_layers) + 1):\n",
    "            if i == 0:\n",
    "                self.__weighted_sum[i] = np.matmul(self.__weights[i].T, input) + self.__biases[i]\n",
    "                self.__activated_output[i] = self.__hidden_layers[i].activate(self.__weighted_sum[i])\n",
    "            elif i == len(self.__hidden_layers):\n",
    "                self.__weighted_sum[i] = np.matmul(self.__weights[i].T, self.__activated_output[i-1]) + self.__biases[i]\n",
    "                if self.__output_layer:\n",
    "                    self.__activated_output[i] = self.__output_layer.activate(self.__weighted_sum[i])\n",
    "                else:\n",
    "                    self.__activated_output[i] = self.__weighted_sum[i]\n",
    "            else:\n",
    "                self.__weighted_sum[i] = np.matmul(self.__weights[i].T, self.__activated_output[i-1]) + self.__biases[i]\n",
    "                self.__activated_output[i] = self.__hidden_layers[i].activate(self.__weighted_sum[i])\n",
    "\n",
    "    def __backpropagate(self, target):\n",
    "        loss_gradient = self.__loss_function.gradient(target, self.activated_output[-1])\n",
    "        \n",
    "        for i in range(len(self.__hidden_layers), -1, -1):\n",
    "            if i == len(self.__hidden_layers):\n",
    "                self.__delta_error[i] = np.matmul(self.__output_layer.gradient(self.__weighted_sum[i]), loss_gradient)\n",
    "            else:\n",
    "                self.__delta_error[i] = np.matmul(self.__weights[i+1], self.__delta_error[i+1])\n",
    "                self.__delta_error[i] = np.matmul(self.__hidden_layers[i].gradient(self.__weighted_sum[i]), self.__delta_error[i])\n",
    "\n",
    "    def __updateWeightsAndBiases(self, input, learning_rate):\n",
    "        for i in range(len(self.__hidden_layers) + 1):\n",
    "            if i == 0:\n",
    "                self.__weights[i] = self.__weights[i] - learning_rate * np.matmul(input, self.__delta_error[i].T)\n",
    "            else:\n",
    "                self.__weights[i] = self.__weights[i] - learning_rate * np.matmul(self.__activated_output[i-1], self.__delta_error[i].T)\n",
    "\n",
    "            self.__biases[i] = self.__biases[i] - learning_rate * self.__delta_error[i]\n",
    "\n",
    "    def __process(self, input, target, learning_rate):\n",
    "        self.__forwardPass(input)\n",
    "        loss = self.__loss_function.loss(target, self.__activated_output[-1])\n",
    "        self.__backpropagate(target)\n",
    "        self.__updateWeightsAndBiases(input, learning_rate)\n",
    "        return loss\n",
    "\n",
    "    def fit(self, train_dataset, target_dataset, learning_rate : float = 0.05, epochs : int = 100):\n",
    "        n = len(train_dataset)\n",
    "        for e in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for i in range(n):\n",
    "                p = np.array(np.expand_dims(train_dataset[i],1), dtype=np.float64)\n",
    "                o = np.zeros((self.__number_of_outputs, 1), dtype=np.float64)\n",
    "                o[int(target_dataset[i][0])] = 1.0\n",
    "                loss = self.__process(p, o, learning_rate)\n",
    "                total_loss += loss\n",
    "                print(f\"Epoch {e + 1}, oveservation {i + 1} : loss = {loss}\", end=\"\\r\", flush=True)\n",
    "            print(f\"Epoch {e + 1}, oveservation {n} : loss = {total_loss / n}\", flush=True)\n",
    "\n",
    "    def predict(self, test_dataset):\n",
    "        pred = []\n",
    "        for i in range(len(test_dataset)):\n",
    "            input = np.array(np.expand_dims(test_dataset[i],1), dtype=np.float64)\n",
    "            self.__forwardPass(input)\n",
    "            pred.append(self.__activated_output[-1])\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    nn = NeuralNetwork(2, 2, MeanSqureLoss(), SoftmaxActivation())\n",
    "\n",
    "    nn.addHiddenLayer(Layer(2, SigmoidActivation()))\n",
    "\n",
    "    nn.initialize()\n",
    "\n",
    "    # weights = [np.array([[0.15, 0.25],\n",
    "    #             [0.20, 0.30]]),\n",
    "    #         np.array([[0.40, 0.50],\n",
    "    #             [0.45, 0.55]])]\n",
    "\n",
    "    # biases = [np.array([[0.35],\n",
    "    #             [0.35]]),\n",
    "    #         np.array([[0.60],\n",
    "    #                     [0.60]])]\n",
    "\n",
    "    # nn.initializeWeights(weights)\n",
    "    # nn.initializeBiases(biases)\n",
    "\n",
    "    x_input = np.array([[0.05],[0.10]], dtype=np.float64)\n",
    "    y_output = np.array([[0.01],[0.99]], dtype=np.float64)\n",
    "    nn.fit(x_input, y_output, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
